{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b25fa997-590c-4834-a637-7d057b4d69a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scapy\n",
      "  Downloading scapy-2.5.0.tar.gz (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Building wheels for collected packages: scapy\n",
      "  Building wheel for scapy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for scapy: filename=scapy-2.5.0-py2.py3-none-any.whl size=1444347 sha256=ddda180bd44f95105d7853243c247c1b2584b5d32f86719245fb90fc9efd5da6\n",
      "  Stored in directory: /home/seojin929_gmail_com/.cache/pip/wheels/82/b7/03/8344d8cf6695624746311bc0d389e9d05535ca83c35f90241d\n",
      "Successfully built scapy\n",
      "Installing collected packages: scapy\n",
      "Successfully installed scapy-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scapy numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a6eb935-655b-4807-a6e7-ab1536589da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: networkx\n",
      "Successfully installed networkx-3.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a27e6f9-0f78-4269-ab43-205c9e8ba6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 0 assigned to cluster 0 with score: 0.504\n",
      "Graph 1 assigned to cluster 0 with score: 0.33599999999999997\n",
      "Graph 2 assigned to cluster 0 with score: 0.33599999999999997\n",
      "Graph 3 assigned to cluster 0 with score: 0.505\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Parameters Setup ---\n",
    "CHUNK_LENGTH = 50  # Length of shingles to be used when hashing the graphs.\n",
    "L = 1000  # Number of hash functions (sketch size).\n",
    "SEED = 23  # Seed for random number generation to ensure reproducibility.\n",
    "CLUSTER_UPDATE_INTERVAL = 10000  # Number of edges after which clusters are updated.\n",
    "B = 100  # Number of hash bands used in Locality-Sensitive Hashing (not used here, but part of LSH).\n",
    "R = 20  # Number of rows in each band in LSH.\n",
    "PI = 3.1415926535897  # Approximate value of PI, could be used in similarity computations (but not in this script).\n",
    "\n",
    "# --- Initialize Random Vectors for StreamHash ---\n",
    "np.random.seed(SEED)  # Set the random seed to make the process reproducible.\n",
    "MAX_UINT64 = np.iinfo(np.uint64).max  # Get the maximum value for an unsigned 64-bit integer (used for random hash generation).\n",
    "# Generate L random vectors, each with CHUNK_LENGTH + 2 elements (64-bit integers). This serves as the universal hash family H.\n",
    "H = [np.random.randint(0, MAX_UINT64, CHUNK_LENGTH+2, dtype=np.uint64) for _ in range(L)]\n",
    "\n",
    "\n",
    "# --- Function to Hash Shingles ---\n",
    "# This function creates a hash for each shingle (a substructure of the graph). Shingles are combinations of node and edge types.\n",
    "def hash_shingle(shingle, randbits):\n",
    "    # Initialize sum_hash as the first random bit.\n",
    "    sum_hash = int(randbits[0])\n",
    "    # For each character in the shingle, update the sum_hash using a unique random number from randbits.\n",
    "    for i, char in enumerate(shingle):\n",
    "        sum_hash += int(randbits[i+1]) * ord(char)  # Multiply character code (ord) with a random bit.\n",
    "    \n",
    "    # The result is right-shifted by 63 to get the most significant bit (which we use as a hash).\n",
    "    # If the MSB is 1, return +1; if 0, return -1 (this gives a binary hash).\n",
    "    return 2 * ((sum_hash >> 63) & 1) - 1\n",
    "\n",
    "\n",
    "# --- Function to Create a StreamHash Sketch for a Graph ---\n",
    "# Constructs a sketch (a compact representation) of a graph using StreamHash. Each graph is represented by its set of shingles.\n",
    "def construct_streamhash_sketch(shingle_vector):\n",
    "    projection = np.zeros(L, dtype=int)  # Initialize the projection vector (length L) for the graph's shingle hashes.\n",
    "    \n",
    "    # For each shingle in the graph, apply the hash function and accumulate its contribution in the projection vector.\n",
    "    for shingle, count in shingle_vector.items():\n",
    "        for i in range(L):\n",
    "            projection[i] += count * hash_shingle(shingle, H[i])  # Multiply shingle occurrence with its hash value.\n",
    "    \n",
    "    # Convert the projection into a binary sketch (0s and 1s). If projection[i] >= 0, set sketch[i] to 1; else, set it to 0.\n",
    "    sketch = np.where(projection >= 0, 1, 0)\n",
    "    return sketch, projection  # Return both the sketch and the projection vector for further use.\n",
    "\n",
    "\n",
    "# --- Function to Compute Similarity Between Two Sketches ---\n",
    "# Compares two sketches by computing the fraction of matching bits (similarity score between 0 and 1).\n",
    "def streamhash_similarity(sketch1, sketch2):\n",
    "    return np.sum(sketch1 == sketch2) / L  # Compute how many bits are the same and divide by the sketch length.\n",
    "\n",
    "\n",
    "# --- Function to Process Edges Into Graphs ---\n",
    "# Reads an edge file and constructs graphs from it. Each graph is a collection of nodes connected by edges.\n",
    "def process_edges(edge_file):\n",
    "    graphs = defaultdict(nx.DiGraph)  # Initialize a dictionary to hold multiple directed graphs (DiGraph).\n",
    "    \n",
    "    # Open the edge file and read it line by line. Each line represents an edge between two nodes in a graph.\n",
    "    with open(edge_file, 'r') as f:\n",
    "        for line in f:\n",
    "            # Extract source node, source type, destination node, destination type, edge type, and graph ID from the line.\n",
    "            src_id, src_type, dst_id, dst_type, e_type, gid = line.strip().split()\n",
    "            # Add an edge between src_id and dst_id in the graph gid.\n",
    "            graphs[int(gid)].add_edge((src_id, src_type), (dst_id, dst_type), e_type=e_type)\n",
    "    \n",
    "    return graphs  # Return a dictionary of graphs.\n",
    "\n",
    "\n",
    "# --- Function to Create Shingle Vectors for Each Graph ---\n",
    "# Constructs shingle vectors for each graph, where each shingle is a combination of source type, edge type, and destination type.\n",
    "def construct_shingle_vectors(graphs):\n",
    "    shingle_vectors = {}  # Initialize a dictionary to store the shingle vector for each graph.\n",
    "    \n",
    "    # For each graph, iterate over its edges and construct shingles.\n",
    "    for gid, graph in graphs.items():\n",
    "        shingle_vector = defaultdict(int)  # Initialize a dictionary to store shingles and their counts.\n",
    "        \n",
    "        # For each edge in the graph, extract the source type, edge type, and destination type to form a shingle.\n",
    "        for u, v, data in graph.edges(data=True):\n",
    "            src_type, dst_type = u[1], v[1]  # Get the types of the source and destination nodes.\n",
    "            shingle = f\"{src_type}{data['e_type']}{dst_type}\"  # Create the shingle as a string.\n",
    "            shingle_vector[shingle] += 1  \n",
    "        # Store the shingle vector for the current graph.\n",
    "        shingle_vectors[gid] = shingle_vector  \n",
    "    # Return a dictionary of shingle vectors for all graphs.\n",
    "    return shingle_vectors  \n",
    "\n",
    "\n",
    "# --- Function to Bootstrap Clusters ---\n",
    "# Reads the initial clusters and thresholds from a file to initialize clusters for the anomaly detection.\n",
    "def read_bootstrap_clusters(bootstrap_file):\n",
    "    # Initialize a dictionary to hold clusters (lists of graph IDs).\n",
    "    clusters = defaultdict(list)  \n",
    "    \n",
    "    # Open the bootstrap cluster file and read the first line to get the number of clusters and the global threshold.\n",
    "    with open(bootstrap_file, 'r') as f:\n",
    "        # Global threshold is used for anomaly detection.\n",
    "        nclusters, global_threshold = map(float, f.readline().split())  \n",
    "        \n",
    "        # Read the subsequent lines to map each graph to its corresponding cluster.\n",
    "        for line in f:\n",
    "            # Read the cluster-specific threshold and graph ID.\n",
    "            threshold, gid = map(float, line.split())  \n",
    "            # Add the graph ID to the cluster corresponding to this threshold.\n",
    "            clusters[int(threshold)].append(int(gid))  \n",
    "    # Return the clusters and the global threshold.\n",
    "    return clusters, global_threshold  \n",
    "\n",
    "\n",
    "# --- Anomaly Detection ---\n",
    "# Computes the distance between each graph's sketch and the cluster centroids, and flags anomalies if the distance is too high.\n",
    "def update_distances_and_clusters(gid, graph_sketches, centroid_sketches, global_threshold):\n",
    "     # Retrieve the sketch for the graph.\n",
    "    sketch = graph_sketches[gid] \n",
    "    # Initialize the minimum distance as the maximum possible (1.0).\n",
    "    min_distance = 1.0  \n",
    "     # Initialize the nearest cluster as None\n",
    "    nearest_cluster = None \n",
    "    \n",
    "    # Iterate through each cluster and compute the similarity between the graph's sketch and the cluster's centroid sketch.\n",
    "    for cluster_id, centroid_sketch in centroid_sketches.items():\n",
    "        # Compute the similarity (between 0 and 1).\n",
    "        sim = streamhash_similarity(sketch, centroid_sketch)  \n",
    "         # Convert similarity to distance (closer to 0 is better).\n",
    "        distance = 1.0 - sim \n",
    "        \n",
    "        # If this distance is smaller than the current minimum, update the minimum distance and the nearest cluster.\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            nearest_cluster = cluster_id\n",
    "    \n",
    "    # If the distance is greater than the global threshold, the graph is flagged as an anomaly.\n",
    "    if min_distance > global_threshold:\n",
    "        print(f\"Graph {gid} is an anomaly with score: {min_distance}\")\n",
    "    else:\n",
    "        # Otherwise, the graph is assigned to the nearest cluster.\n",
    "        print(f\"Graph {gid} assigned to cluster {nearest_cluster} with score: {min_distance}\")\n",
    "\n",
    "\n",
    "# --- Main Function ---\n",
    "def main():\n",
    "    # Step 1: Read the edge file and bootstrap clusters.\n",
    "    # Load graphs from the edge file.\n",
    "    graphs = process_edges('test_edges.txt')  \n",
    "    # Load clusters and threshold.\n",
    "    clusters, global_threshold = read_bootstrap_clusters('test_bootstrap_clusters.txt')  \n",
    "\n",
    "    # Step 2: Create shingle vectors and StreamHash sketches for all graphs.\n",
    "    # Construct shingles for each graph.\n",
    "    shingle_vectors = construct_shingle_vectors(graphs)  \n",
    "    # Initialize a dictionary to hold the sketches for each graph.\n",
    "    graph_sketches = {}  \n",
    "    \n",
    "    # For each graph, generate its StreamHash sketch from the shingle vector.\n",
    "    for gid, shingle_vector in shingle_vectors.items():\n",
    "         # Create the sketch for the graph.\n",
    "        sketch, projection = construct_streamhash_sketch(shingle_vector) \n",
    "        # Store the sketch in the dictionary\n",
    "        graph_sketches[gid] = sketch  .\n",
    "    \n",
    "    # Step 3: Initialize cluster centroids (average sketches for each cluster).\n",
    "    # Dictionary to store centroid sketches for each cluster.\n",
    "    centroid_sketches = {}  \n",
    "    for cluster_id, gids in clusters.items():\n",
    "        # Compute average projection.\n",
    "        centroid_projection = np.mean([graph_sketches[gid] for gid in gids], axis=0)  \n",
    "        # Convert average projection to a binary sketch.\n",
    "        centroid_sketch = np.where(centroid_projection >= 0, 1, 0)  \n",
    "         # Store the centroid sketch.\n",
    "        centroid_sketches[cluster_id] = centroid_sketch \n",
    "    \n",
    "    # Step 4: Perform anomaly detection for all graphs.\n",
    "    for gid in graph_sketches.keys():\n",
    "        update_distances_and_clusters(gid, graph_sketches, centroid_sketches, global_threshold)  # Detect anomalies.\n",
    "\n",
    "# --- Entry Point ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute the main function when the script is run.\n",
    "    main()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb3f61e-cf42-4bd1-bdef-89e68fc0fead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
