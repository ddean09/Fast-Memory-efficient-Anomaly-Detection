{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b25fa997-590c-4834-a637-7d057b4d69a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scapy\n",
      "  Downloading scapy-2.5.0.tar.gz (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Building wheels for collected packages: scapy\n",
      "  Building wheel for scapy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for scapy: filename=scapy-2.5.0-py2.py3-none-any.whl size=1444347 sha256=ddda180bd44f95105d7853243c247c1b2584b5d32f86719245fb90fc9efd5da6\n",
      "  Stored in directory: /home/seojin929_gmail_com/.cache/pip/wheels/82/b7/03/8344d8cf6695624746311bc0d389e9d05535ca83c35f90241d\n",
      "Successfully built scapy\n",
      "Installing collected packages: scapy\n",
      "Successfully installed scapy-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scapy numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a6eb935-655b-4807-a6e7-ab1536589da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: networkx\n",
      "Successfully installed networkx-3.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a27e6f9-0f78-4269-ab43-205c9e8ba6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Parameters Setup ---\n",
    "CHUNK_LENGTH = 50  # Length of shingles to be used when hashing the graphs.\n",
    "L = 1000  # Number of hash functions (sketch size).\n",
    "SEED = 23  # Seed for random number generation to ensure reproducibility.\n",
    "CLUSTER_UPDATE_INTERVAL = 10000  # Number of edges after which clusters are updated.\n",
    "GLOBAL_THRESHOLD = 0.75  # Anomaly threshold\n",
    "CLUSTER_THRESHOLD = 0.5  # Threshold for clustering\n",
    "MAX_EDGES = 5000  # Maximum number of edges to process before clustering.\n",
    "\n",
    "# --- Initialize Random Vectors for StreamHash ---\n",
    "np.random.seed(SEED)\n",
    "MAX_UINT64 = np.iinfo(np.uint64).max  # Get the maximum value for an unsigned 64-bit integer.\n",
    "H = [np.random.randint(0, MAX_UINT64, CHUNK_LENGTH+2, dtype=np.uint64) for _ in range(L)]\n",
    "\n",
    "\n",
    "# --- Function to Hash Shingles ---\n",
    "def hash_shingle(shingle, randbits):\n",
    "    sum_hash = int(randbits[0])\n",
    "    for i, char in enumerate(shingle):\n",
    "        sum_hash += int(randbits[i+1]) * ord(char)\n",
    "    return 2 * ((sum_hash >> 63) & 1) - 1\n",
    "\n",
    "\n",
    "# --- Function to Create a StreamHash Sketch for a Graph ---\n",
    "def update_streamhash_sketch(projection, shingle_vector):\n",
    "    # Update the projection with new shingles as they come in.\n",
    "    for shingle, count in shingle_vector.items():\n",
    "        for i in range(L):\n",
    "            projection[i] += count * hash_shingle(shingle, H[i])\n",
    "    # Convert projection to sketch\n",
    "    sketch = np.where(projection >= 0, 1, 0)\n",
    "    return sketch, projection\n",
    "\n",
    "\n",
    "# --- Function to Process Edges into Graphs and Update Sketches ---\n",
    "def process_edge(graphs, shingle_vectors, projections, edge_line):\n",
    "    # Extract edge info\n",
    "    src_id, src_type, dst_id, dst_type, e_type, gid = edge_line.strip().split()\n",
    "    gid = int(gid)  # Graph ID\n",
    "    graph = graphs[gid]  # Retrieve the graph for the given ID\n",
    "    \n",
    "    # Add edge to the graph\n",
    "    graph.add_edge((src_id, src_type), (dst_id, dst_type), e_type=e_type)\n",
    "    \n",
    "    # Create/update shingles for this edge\n",
    "    shingle = f\"{src_type}{e_type}{dst_type}\"\n",
    "    shingle_vectors[gid][shingle] += 1\n",
    "    \n",
    "    # Update the sketch incrementally for this graph\n",
    "    sketch, projection = update_streamhash_sketch(projections[gid], shingle_vectors[gid])\n",
    "    return sketch\n",
    "\n",
    "\n",
    "# --- Function to Compute Similarity Between Two Sketches ---\n",
    "def streamhash_similarity(sketch1, sketch2):\n",
    "    return np.sum(sketch1 == sketch2) / L\n",
    "\n",
    "\n",
    "# --- Function for Locality-Sensitive Hashing (LSH) ---\n",
    "def lsh_buckets(sketch):\n",
    "    # LSH for grouping similar sketches\n",
    "    band_size = 10  # Band size to split the sketch into bands\n",
    "    hash_buckets = []\n",
    "    for i in range(0, L, band_size):\n",
    "        band = tuple(sketch[i:i+band_size])  # Hash each band (10 bits)\n",
    "        hash_buckets.append(hash(band))  # Store hash of each band\n",
    "    return hash_buckets\n",
    "\n",
    "\n",
    "# --- Online Clustering ---\n",
    "def update_clusters(gid, graph_sketches, centroid_sketches, global_threshold, clusters):\n",
    "    sketch = graph_sketches[gid]  # Get sketch of the current graph\n",
    "    min_distance = 1.0\n",
    "    nearest_cluster = None\n",
    "    \n",
    "    # Compare the sketch to each cluster centroid to find the nearest cluster\n",
    "    for cluster_id, centroid_sketch in centroid_sketches.items():\n",
    "        sim = streamhash_similarity(sketch, centroid_sketch)\n",
    "        distance = 1.0 - sim\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            nearest_cluster = cluster_id\n",
    "    \n",
    "    # If the graph is too far from all centroids, it is flagged as an anomaly\n",
    "    if min_distance > global_threshold:\n",
    "        print(f\"Graph {gid} is an anomaly with score: {min_distance}\")\n",
    "    else:\n",
    "        # Otherwise, assign the graph to the nearest cluster\n",
    "        print(f\"Graph {gid} assigned to cluster {nearest_cluster} with score: {min_distance}\")\n",
    "        clusters[nearest_cluster].append(gid)  # Add the graph to the nearest cluster\n",
    "    return nearest_cluster, min_distance\n",
    "\n",
    "\n",
    "# --- Main Function to Process Streaming Edges and Perform Clustering/Anomaly Detection ---\n",
    "def main():\n",
    "    graphs = defaultdict(nx.DiGraph)  # Store multiple graphs\n",
    "    shingle_vectors = defaultdict(lambda: defaultdict(int))  # Shingle vectors for each graph\n",
    "    projections = defaultdict(lambda: np.zeros(L, dtype=int))  # Sketch projection vectors for each graph\n",
    "    graph_sketches = {}  # Store the sketch for each graph\n",
    "    centroid_sketches = {}  # Store the centroid sketches for each cluster\n",
    "    clusters = defaultdict(list)  # Store clusters of graphs\n",
    "    edge_count = 0  # Count the number of edges processed\n",
    "    \n",
    "    # Open the edge file (streaming edges)\n",
    "    with open('test_edges.txt', 'r') as f:\n",
    "        for edge_line in f:\n",
    "            edge_count += 1\n",
    "            # Process each edge and update the corresponding graph and its sketch\n",
    "            sketch = process_edge(graphs, shingle_vectors, projections, edge_line)\n",
    "            gid = int(edge_line.strip().split()[-1])  # Graph ID\n",
    "            \n",
    "            # Store the sketch for the graph\n",
    "            graph_sketches[gid] = sketch\n",
    "            \n",
    "            # Perform clustering/anomaly detection every MAX_EDGES\n",
    "            if edge_count % MAX_EDGES == 0:\n",
    "                print(f\"\\nProcessing Clusters after {edge_count} edges...\")\n",
    "                \n",
    "                # Update the centroid sketches for each cluster\n",
    "                for cluster_id, gids in clusters.items():\n",
    "                    centroid_projection = np.mean([projections[gid] for gid in gids], axis=0)\n",
    "                    centroid_sketch = np.where(centroid_projection >= 0, 1, 0)\n",
    "                    centroid_sketches[cluster_id] = centroid_sketch\n",
    "                \n",
    "                # Check and update clusters for all graphs processed so far\n",
    "                for gid in graph_sketches:\n",
    "                    update_clusters(gid, graph_sketches, centroid_sketches, GLOBAL_THRESHOLD, clusters)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67482750-3f04-4494-87a1-a2c9e1a277b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
